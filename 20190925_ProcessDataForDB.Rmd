---
jupyter:
  jupytext:
    formats: ipynb,Rmd
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.1'
      jupytext_version: 1.2.4
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

# ReadMe

**This is the notebook to import the data from K/I/whatever the driver is and send raw/cleaned data to a postgresql DB on powerplant.**

**Plan:**

1. Read data in
2. Select down to right TDR measurments
3. Send the raw data from each tdr to DB - this is for examing the suprise values 
4. Clean out missing and stupid high values
5. Average the value in each layer (7 layers in total)
6. Calculate deficit 
7. Upload the calculated deficit to DB - for real irrigation scheduling - need to be a separate table 

**PostgreSQL credentials**

    host = "database.powerplant.pfr.co.nz",
    database = "cflfcl_Rainshelter_SWC",
    user = "cflfcl_Rainshelter_SWC",
    password = "o654UkI6iGNwhzHu"
    
**Format that `sqlalchemy` like**
    
    "postgresql://cflfcl_Rainshelter_SWC:o654UkI6iGNwhzHu@database.powerplant.pfr.co.nz/cflfcl_Rainshelter_SWC"
    
**Demo data source**

    K:\Rainshelter\StonySoilLysimeters

# The initial data base test 
### libraries
```{r connector}
library(reticulate)
```


```{python}
import datetime
import pandas as pd
import numpy as np
import time 
import psycopg2
from sqlalchemy import create_engine
```

### DataSource

```{python}
path="K:/Rainshelter/StonySoilLysimeters/"
```

```{python}
# Read in the main data
AllData=pd.read_csv(path + 'DownloadedData/StonyLysimetersCS650.dat', #specify file path for data to read in
                         parse_dates=True, #tell the function to parse date columns to datetime formats
                         dayfirst=True, #tell the function that the day is before the year in the data i.e format='%d/%m/%Y %H:%M'
                         skiprows = [0,2,3], #leave out rows 1, 3 and 4 which have redundant information
                         index_col = 0, #Use the first column, which is Date, as an index
                         na_values = 'NAN')
```

```{python}
AllData.tail()
```


```{python}
#The index for sensors
AllDataIndex = pd.read_excel(path + "Lysometer_design.xlsx",
                             sheet_name="SensorIndex",
                             index_col = 0)
AllDataIndex.head()

```

```{python}
# filter the part that interested in 
FilteredIndex=AllDataIndex[AllDataIndex.Measurement.isin(['VolumetricWaterContent'])] # structure to add in more cols
```

```{python}
FilteredIndex.head()
FilteredIndex.describe()
np.load('LastRow.npy')
```

```{python}
# select only the interested columns 
FilteredData=AllData.loc[:,FilteredIndex.index]
```

```{python}
# set up the index and output the last row 
FilteredDataTrans=FilteredData.transpose() # transpose to the format match the index format
FilteredDataIndexed=pd.concat([FilteredIndex,FilteredDataTrans], axis=1) # join them together

FilteredDataIndexed.index.name='ColumnHeader'
FilteredDataIndexed.set_index(['Measurement','Depth','Gravels','Stones','Column','Sensor', 'MUX', 'Port','Units','Summary','Block','Treatment'], 
                        append=False, inplace=True)
FilteredDataIndexed.sort_index(inplace=True)
FieldData=FilteredDataIndexed.transpose()
LastRow = FieldData.index.size
FieldData.index = pd.to_datetime(FieldData.index) 

np.save('LastRow.npy',LastRow)
FieldData.to_pickle('.\FieldData.pkl')
```

### Save the last ob as a starting point for next up date

```{python}
LastRow=np.load('LastRow.npy')
LastRow
# =163055
FieldData
```

### Summarising 

```{python}
# last filter to get ready upload the raw 
# is not actually calculate any mean, just want to see the data
grouped = FieldData.loc['2018-10-10':].groupby(level=['Sensor'],axis=1).mean().round(2)
grouped
```


## Slice out a piece to do a test 

```{python}
# slice 2000 rows as a test set
rows=list(range(LastRow-672, LastRow))
Test=FieldData.iloc[rows, ]
Test.head()
```

```{python}
# round values to 2 decimals for quick upload
# there must be clear way to group things without mean calculation
Testgrouped = Test.groupby(level=['Depth','Sensor'],axis=1).mean().round(2)

# change the index into single column name
L1 = Testgrouped.columns.get_level_values(0)
L2 = Testgrouped.columns.get_level_values(1)

# rename the columns
Testgrouped.columns= (L1 + '_' + L2)
```


### DB connection and uploading

```{python}
# FieldData.dtypes
# FieldData.index
engine = create_engine("postgresql://cflfcl_Rainshelter_SWC:o654UkI6iGNwhzHu@database.powerplant.pfr.co.nz/cflfcl_Rainshelter_SWC")
Testgrouped.to_sql(name="Test",con=engine,if_exists='replace' ) # will over write db
```



```{python}


skips=[0,2] + list(range(3, LastRow + 4)) 
# the 'LastRow' vector is the last row in the imported data frame 
# not the right one in the raw data 
# use +4 to correct it, might need to pay attention for future bugs. 

```

### try skip reading 
```{python}


appendData=pd.read_csv(path + 'DownloadedData/StonyLysimetersCS650.dat', #specify file path for data to read in
                         parse_dates=True, #tell the function to parse date columns to datetime formats
                         dayfirst=True, #tell the function that the day is before the year in the data i.e format='%d/%m/%Y %H:%M'
                         skiprows = skips, #leave out rows 1, 3 and 4 which have redundant information
                         index_col = 0, #Use the first column, which is Date, as an index
                         na_values = 'NAN')

```
```{python}
# import re
# pattern=r'-07 '
# FAKE a df for appending
# T2.index = T2.index.to_series().astype(str).str.replace(r'-07 ','-08 ')
appendData.index
appendData.columns
LastRow=appendData.index.size + LastRow
LastRow
```

**Updating the DB**


```{python}
# T2.to_sql(name="Test",con=engine,if_exists='append' )
LastRead = np.load('LastRow.npy')
```


## TEST an update fun
```{python}
# def UpdataDataFrame():
#Bring in previous status files
LastRead = np.load('LastRow.npy')
LastRead
FieldData = pd.read_pickle('FieldData.pkl')
FieldData.index.size
np.save()
```

#Bring in index data
AllDataIndex = pd.read_excel(path + "Lysometer_design.xlsx",
                         sheet_name="SensorIndex",
                         index_col = 0)

#Bring in fresh data
StartRead = LastRead + 4
Skips = [0,2,3] + list(range(4,StartRead))
FreshData=pd.read_csv(path + 'DownloadedData/StonyLysimetersCS650.dat', #specify file path for data to read in    
                      parse_dates=True, #tell the function to parse date columns to datetime formats
                      dayfirst=True, #tell the function that the day is before the year in the data i.e format='%d/%m/%Y %H:%M'
                      skiprows = Skips, #rows that have already be read
                      index_col = 0, #Use the first column, which is Date, as an index
                      na_values = 'NAN')

#Apply indexes to fresh data
FilteredIndex=AllDataIndex[AllDataIndex.Measurement.isin(['VolumetricWaterContent'])] # structure to add in more cols
FreshData=FreshData.loc[:,FilteredIndex.index]

FreshDataTransposed = FreshData.transpose()
FreshDataIndexed = pd.concat([FilteredIndex,FreshDataTransposed], axis=1, sort=True)
FreshDataIndexed.index.name='ColumnHeader'
FreshDataIndexed.set_index(['Measurement','Depth','Gravels','Stones','Column','Sensor', 'MUX', 'Port','Units','Summary','Block','Treatment'],
                        append=False, inplace=True) # need to automate
FreshDataIndexed.sort_index(inplace=True)
NewData=FreshDataIndexed.transpose()

NewData.index = pd.to_datetime(NewData.index) 

#Rename the fresh data to concat the colnames 
## round values to 2 decimals for quick upload
## there must be clear way to group things without mean calculation
FieldDatagrouped = NewData.groupby(level=['Depth','Sensor'],axis=1).mean().round(2)

## change the index into single column name
L1 = FieldDatagrouped.columns.get_level_values(0)
L2 = FieldDatagrouped.columns.get_level_values(1)

## rename the columns
FieldDatagrouped.columns= (L1 + '_' + L2)

#Update the DB 
## create the engine for connection
engine = create_engine("postgresql://cflfcl_Rainshelter_SWC:o654UkI6iGNwhzHu@database.powerplant.pfr.co.nz/cflfcl_Rainshelter_SWC")
FieldDatagrouped.to_sql(name="Test",con=engine,if_exists='append' )

#Update status files
LastRow = FieldDatagrouped.index.size + LastRead

```{python}
# np.save('LastRow.npy',LastRow)
np.load('LastRow.npy')
```



# unwanted bits

### Calculate Deficit 

```{python}
DataMeans =  FieldData.loc['2015-10-10':].groupby(level=['Measurement','Depth','Gravels','Stones'],axis=1).mean()
DataMeans =  DataMeans.dropna(axis=1) #For some reason it keeps non valid combinations in so need to extract with this function
ProfileWater = DataMeans.VolumetricWaterContent.loc[:, 'D1'] * 150 + \
               DataMeans.VolumetricWaterContent.loc[:, 'D2'] * 150 + \
               DataMeans.VolumetricWaterContent.loc[:, 'D3'] * 150 + \
               DataMeans.VolumetricWaterContent.loc[:, 'D4'] * 150 
FieldCapacity = ProfileWater.resample('D').max()
FieldCapacity = FieldCapacity.loc['2015-10-14'] +10 # I would have though this would return a data frame with a single row but instead it returns a series with a multiindex in columns
SoilWaterDeficit = -(FieldCapacity - ProfileWater)  
```

```{python}
SoilWaterDeficit.transpose()
```

```{python}
#uploading 
SoilWaterDeficit.to_sql(name="SoilWaterDeficit",con=engine, if_exists ='replace')
```

```{python}
def Update():
    #Update Data Frame with data that has been logged since last update
    FieldData = UpdataDataFrame()
    #Calculate treatment means omitting data prior to 2014-11-05 08:00:00 to avoid NaN values
    DataMeans =  FieldData.ix['2015-10-10':].groupby(level=['Measurement','Depth','Irrigation', 'Nitrogen'],axis=1).mean()
    DataMeans =  DataMeans.dropna(axis=1) #For some reason it keeps non valid combinations in so need to extract with this function
    DataMeans = DataMeans.resample('4H').last()
    
    #Calculate the water content of the soil profile by multiplying the volumetric water content by each layers
    #depth and summing.  The 0-15 layers are divided by 2 to average the two readings
    ProfileWater = DataMeans.VolumetricWaterContent.loc[:, 'D1'] * 150 + \
                   DataMeans.VolumetricWaterContent.loc[:, 'D2'] * 150 + \
                   DataMeans.VolumetricWaterContent.loc[:, 'D3'] * 150 + \
                   DataMeans.VolumetricWaterContent.loc[:, 'D4'] * 150 

    FieldCapacity = ProfileWater.resample('D', how='max')
    FieldCapacity = FieldCapacity.ix['2015-10-14'] +30 # I would have though this would return a data frame with a single row but instead it returns a series with a multiindex in columns
    SoilWaterDeficit = -(FieldCapacity - ProfileWater)    # This calculation only works because field capacity is a multi index series
```

```{python}

```

```{python}
while True:
    try:
        Update() #get fresh data and send to graph
    except:
        retrys = 1
        print('Attempt to stream data failed')
        while retrys < 5: ##retry connecting 4 times
            time.sleep(5)  ##wait 2 min 
            try:
                Update() # have another go
            except:
                print('Retry' + str(retrys) + ' failed')
                retrys +=1 #increment try counter and try again
            else:
                retrys = 6 #set try counter so loop is exited
    time.sleep(10)  ## update again in an hours time
```



```{r, message=FALSE, warning=FALSE}
source("./DBconnection_funs.R")
con = setupDBConn()
dbListTables(con)

dbWriteTable(con, name = "Test", value = py$T1, overwrite = TRUE)

dbWriteTable(con, name = "Test", value = py$T2, append = TRUE) 
sql <- paste0("CREATE INDEX ON Test (row.names);")
  dbExecute(con, sql)
```

```{python}
# slice two sets from the test. one pretends to be the old file, one for update 
# Testgrouped = Testgrouped.dropna(axis=1, how='all')
T1 = Testgrouped.iloc[0 : (Testgrouped.index.size - 10), ]
T2 = Testgrouped.iloc[(Testgrouped.index.size - 10) : Testgrouped.index.size, ]
```

```{python}
# Checking the time 
T1.iloc[(T1.index.size - 1):T1.index.size,]

T2# Checking the time 
```

