{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aim\n",
    "\n",
    "1. Define the new function to update the db\n",
    "\n",
    "\n",
    "_note_ All the component was tested on the `rmd` file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load required pkgs\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time \n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up key path and constant\n",
    "path=\"K:/Rainshelter/StonySoilLysimeters/\"\n",
    "file_lysimeters='DownloadedData/StonyLysimetersCS650.dat'\n",
    "file_index='Lysometer_design.xlsx'\n",
    "sheet_index='SensorIndex'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UpdataDataFrame():\n",
    "    #Bring in previous status files\n",
    "    LastRead = np.load('LastRow.npy')\n",
    "       \n",
    "    #Bring in index data\n",
    "    AllDataIndex = pd.read_excel(path + \"Lysometer_design.xlsx\",\n",
    "                             sheet_name=\"SensorIndex\",\n",
    "                             index_col = 0)\n",
    "    \n",
    "    #Bring in fresh data\n",
    "    # the 'LastRead' vector is the last row in the imported data frame, which already skipped 0,2,3\n",
    "    # not the right one in the raw data,use +4 to correct it, might need to pay attention for future bugs. \n",
    "    StartRead = LastRead + 4\n",
    "    Skips = [0,2,3] + list(range(4,StartRead))\n",
    "    FreshData=pd.read_csv(path + 'DownloadedData/StonyLysimetersCS650.dat', #specify file path for data to read in    \n",
    "                          parse_dates=True, #tell the function to parse date columns to datetime formats\n",
    "                          dayfirst=True, #tell the function that the day is before the year in the data i.e format='%d/%m/%Y %H:%M'\n",
    "                          skiprows = Skips, #rows that have already be read\n",
    "                          index_col = 0, #Use the first column, which is Date, as an index\n",
    "                          na_values = 'NAN')\n",
    "    \n",
    "    #Apply indexes to fresh data\n",
    "    FilteredIndex=AllDataIndex[AllDataIndex.Measurement.isin(['VolumetricWaterContent'])] # structure to add in more cols\n",
    "    FreshData=FreshData.loc[:,FilteredIndex.index]\n",
    "    \n",
    "    FreshDataTransposed = FreshData.transpose()\n",
    "    FreshDataIndexed = pd.concat([FilteredIndex,FreshDataTransposed], axis=1, sort=True)\n",
    "    FreshDataIndexed.index.name='ColumnHeader'\n",
    "    FreshDataIndexed.set_index(['Measurement','Depth','Gravels','Stones','Column','Sensor', 'MUX', 'Port','Units','Summary','Block','Treatment'],\n",
    "                            append=False, inplace=True) # need to automate\n",
    "    FreshDataIndexed.sort_index(inplace=True)\n",
    "    NewData=FreshDataIndexed.transpose()\n",
    "    \n",
    "    NewData.index = pd.to_datetime(NewData.index) \n",
    "    \n",
    "    #Rename the fresh data to concat the colnames \n",
    "    ## round values to 2 decimals for quick upload\n",
    "    ## there must be clear way to group things without mean calculation\n",
    "    FieldDatagrouped = NewData.groupby(level=['Depth','Sensor'],axis=1).mean().round(2)\n",
    "    \n",
    "    ## change the index into single column name\n",
    "    L1 = FieldDatagrouped.columns.get_level_values(0)\n",
    "    L2 = FieldDatagrouped.columns.get_level_values(1)\n",
    "    \n",
    "    ## rename the columns\n",
    "    FieldDatagrouped.columns= (L1 + '_' + L2)\n",
    "    \n",
    "    \n",
    "    #Calculate mean value based on the type of the lysimeters\n",
    "    DataMeans = NewData.loc['2018-10-10':].groupby(level=['Measurement','Depth','Gravels','Stones'],axis=1).mean()\n",
    "    DataMeans =  DataMeans.dropna(axis=1) #For some reason it keeps non valid combinations in so need to extract with this function\n",
    "    DataMeans = DataMeans.resample('4H').last() # 4 hours subsetting\n",
    "\n",
    "    #Calculate the water content of the soil profile by multiplying the volumetric water content by each layers\n",
    "    #depth and summing.  The 0-15 layers are divided by 2 to average the two readings\n",
    "    ProfileWater = DataMeans.VolumetricWaterContent.loc[:, 'D1'] * 150 + \\\n",
    "                   DataMeans.VolumetricWaterContent.loc[:, 'D2'] * 150 + \\\n",
    "                   DataMeans.VolumetricWaterContent.loc[:, 'D3'] * 150 + \\\n",
    "                   DataMeans.VolumetricWaterContent.loc[:, 'D4'] * 150 \n",
    "\n",
    "    #Extrat values from the index for single col names\n",
    "    L1 = ProfileWater.columns.get_level_values(0)\n",
    "    L2 = ProfileWater.columns.get_level_values(1)\n",
    "\n",
    "    # rename the columns\n",
    "    ProfileWater.columns = L1 + '_' + L2\n",
    "\n",
    "    FieldCapacity = ProfileWater.resample('D').last()\n",
    "    SoilWaterDeficit = -(FieldCapacity - ProfileWater)\n",
    "\n",
    "    #Upload results to DB\n",
    "    ## create the engine for connection\n",
    "    engine = create_engine(\"postgresql://cflfcl_Rainshelter_SWC:o654UkI6iGNwhzHu@database.powerplant.pfr.co.nz/cflfcl_Rainshelter_SWC\")\n",
    "    \n",
    "    ## All sensors\n",
    "    FieldDatagrouped.to_sql(name=\"Test\",con=engine,if_exists='append' )\n",
    "    \n",
    "    ## Summarised data\n",
    "    SoilWaterDeficit.dropna().to_sql(name=\"SoilWaterDeficit\",con=engine,if_exists='append' ) # append to db\n",
    "    ProfileWater.to_sql(name=\"ProfileWater\",con=engine,if_exists='append' ) # append to db\n",
    "       \n",
    "    #Update status files\n",
    "    LastRow = FieldDatagrouped.index.size + LastRead\n",
    "    np.save('LastRow.npy',LastRow)\n",
    "    NewData.to_pickle('.\\FieldData.pkl')\n",
    "    return FieldData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt to stream data failed\n",
      "Retry1 failed\n",
      "Retry2 failed\n",
      "Retry3 failed\n",
      "Retry4 failed\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    try:\n",
    "        UpdataDataFrame() #get fresh data and send to db\n",
    "    except:\n",
    "        retrys = 1\n",
    "        print('Attempt to stream data failed')\n",
    "        while retrys < 5: ##retry connecting 4 times\n",
    "            time.sleep(450)  ##wait 2 min \n",
    "            try:\n",
    "                Update() # have another go\n",
    "            except:\n",
    "                print('Retry' + str(retrys) + ' failed')\n",
    "                retrys +=1 #increment try counter and try again\n",
    "            else:\n",
    "                retrys = 6 #set try counter so loop is exited\n",
    "    time.sleep(900) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
